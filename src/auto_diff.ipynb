{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28332d6a-857a-42a7-94aa-9c0181e1cfce",
      "metadata": {
        "id": "28332d6a-857a-42a7-94aa-9c0181e1cfce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import sin, cos\n",
        "import math\n",
        "from numpy import log\n",
        "from typing import List, Tuple\n",
        "from numpy.testing import assert_almost_equal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca601b9-cb3e-459b-bc23-847c4b08b600",
      "metadata": {
        "id": "2ca601b9-cb3e-459b-bc23-847c4b08b600"
      },
      "source": [
        "# forward mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "161f3fea-1724-4e00-bc91-64ddced37337",
      "metadata": {
        "id": "161f3fea-1724-4e00-bc91-64ddced37337"
      },
      "outputs": [],
      "source": [
        "class ValueFwd:\n",
        "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
        "    def __init__(self, data, diff = 0):\n",
        "        self.v = data\n",
        "        self.d = diff\n",
        "\n",
        "    def __add__(l, r):\n",
        "        \"\"\" support for + on ValueFwd datatype.\n",
        "\n",
        "         l + r returns f, such that f.v = l.v + r.v and f.d = f' = df/dx.\n",
        "\n",
        "        Args:\n",
        "            l (ValueFwd): ValueFwd to the left of +.\n",
        "            r (ValueFwd): ValueFwd to the right of +.\n",
        "\n",
        "        Returns:\n",
        "            ValueFwd with updated value and gradient.\n",
        "        \"\"\"\n",
        "        return ValueFwd(l.v + r.v, l.d + r.d)\n",
        "\n",
        "    def __sub__(l, r):\n",
        "        \"\"\" support for - on ValueFwd datatype.\n",
        "\n",
        "         l - r returns f, such that f.v = l.v - r.v and f.d = f' = df/dx.\n",
        "\n",
        "        Args:\n",
        "            l (ValueFwd): ValueFwd to the left of -.\n",
        "            r (ValueFwd): ValueFwd to the right of -.\n",
        "\n",
        "        Returns:\n",
        "            ValueFwd with updated value and gradient.\n",
        "        \"\"\"\n",
        "        return ValueFwd(l.v - r.v, l.d - r.d)\n",
        "\n",
        "    def __mul__(l, r):\n",
        "        \"\"\" support for * on ValueFwd datatype.\n",
        "\n",
        "         l * r returns f, such that f.v = l.v * r.v and f.d = f' = df/dx.\n",
        "\n",
        "        Args:\n",
        "            l (ValueFwd): ValueFwd to the left of *.\n",
        "            r (ValueFwd): ValueFwd to the right of *.\n",
        "\n",
        "        Returns:\n",
        "            ValueFwd with updated value and gradient.\n",
        "        \"\"\"\n",
        "        return ValueFwd(l.v * r.v, l.d * r.v + l.v * r.d)\n",
        "\n",
        "    def __truediv__(l, r):\n",
        "        \"\"\" support for / on ValueFwd datatype.\n",
        "\n",
        "         l / r returns f, such that f.v = l.v / r.v and f.d = f' = df/dx.\n",
        "\n",
        "        Args:\n",
        "            l (ValueFwd): ValueFwd to the left of /.\n",
        "            r (ValueFwd): ValueFwd to the right of /.\n",
        "\n",
        "        Returns:\n",
        "            ValueFwd with updated value and gradient.\n",
        "        \"\"\"\n",
        "        return ValueFwd(l.v / r.v, ((l.d * r.v - l.v * r.d) / r.v**2))\n",
        "\n",
        "    def sin(self):\n",
        "        \"\"\" support for self.sin() on ValueFwd datatype.\n",
        "\n",
        "        Args:\n",
        "            self (ValueFwd): ValueFwd.\n",
        "\n",
        "        Returns:\n",
        "            returns ValueFwd f, such that f.v = sin(self.v) and f.d = f' = df/dx\n",
        "        \"\"\"\n",
        "        return ValueFwd(math.sin(self.v), math.cos(self.v) * self.d)\n",
        "\n",
        "    def cos(self):\n",
        "        \"\"\" support for self.cos() on ValueFwd datatype.\n",
        "\n",
        "        Args:\n",
        "            self (ValueFwd): ValueFwd.\n",
        "\n",
        "        Returns:\n",
        "            returns ValueFwd f, such that f.v = cos(self.v) and f.d = f' = df/dx\n",
        "        \"\"\"\n",
        "        return ValueFwd(math.cos(self.v), -math.sin(self.v) * self.d)\n",
        "\n",
        "    def exp(self, b):\n",
        "        \"\"\" support for b^self on ValueFwd datatype.\n",
        "\n",
        "            self.exp(math.e) = e^self\n",
        "\n",
        "        Args:\n",
        "            self (ValueFwd): ValueFwd.\n",
        "            b (Number): A numerical value.\n",
        "\n",
        "        Returns:\n",
        "            returns ValueFwd f, such that f.v = b^(self.v) and f.d = f' = df/dx\n",
        "        \"\"\"\n",
        "        return ValueFwd(b**self.v, (b**self.v) * math.log(b) * self.d)\n",
        "\n",
        "    def pow(self, p):\n",
        "        \"\"\" support for pow on ValueFwd datatype.\n",
        "\n",
        "            self.pow(3) = self^3\n",
        "\n",
        "        Args:\n",
        "            self (ValueFwd): ValueFwd.\n",
        "            p (Number): A numerical value.\n",
        "\n",
        "        Returns:\n",
        "            returns ValueFwd f, such that f.v = (self.v)^p and f.d = f' = df/dx\n",
        "        \"\"\"\n",
        "        return ValueFwd(self.v**p, p * (self.v**(p-1)) * self.d)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"v: {self.v}, d:{self.d}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7594c5-91b7-4cdb-90f2-912d0c5da30d",
      "metadata": {
        "id": "dd7594c5-91b7-4cdb-90f2-912d0c5da30d"
      },
      "source": [
        "# Test forward-mode automatic differentiation\n",
        "\n",
        "Test function 1 is: `test_f1(x)`\n",
        "$$f(x) = \\sin\\left( \\frac{\\sqrt{e^x + 2}}{2}\\right)$$\n",
        "\n",
        "The gradient for Test 1 is: `test_df1`\n",
        "$$\\frac{e^x \\cos\\left( \\frac{\\sqrt{e^x + 2}}{2} \\right)}{4 \\sqrt{e^x + 2}}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "55291991-2c18-47de-a91f-b330ede140bb",
      "metadata": {
        "id": "55291991-2c18-47de-a91f-b330ede140bb"
      },
      "outputs": [],
      "source": [
        "def test_f1(x):\n",
        "    return ((x.exp(math.e) + ValueFwd(2)).pow(0.5)/ValueFwd(2)).sin()\n",
        "\n",
        "def test_df1(x):\n",
        "    ex = math.exp(x)\n",
        "    num = ex*cos(math.sqrt(ex+2)/2)\n",
        "    den = 4 * math.sqrt(ex+2)\n",
        "    return num/den"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca95c0d-aba3-4cee-b033-2acfe8e8c52d",
      "metadata": {
        "id": "4ca95c0d-aba3-4cee-b033-2acfe8e8c52d"
      },
      "source": [
        "## test Froward-mode Automatic Differentiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "da485978-28e2-489f-9886-c7d0f06d1704",
      "metadata": {
        "id": "da485978-28e2-489f-9886-c7d0f06d1704"
      },
      "outputs": [],
      "source": [
        "assert_almost_equal(test_f1(ValueFwd(1, 1)).d, test_df1(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3oWm4mJUzAqo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oWm4mJUzAqo",
        "outputId": "888a7a0e-4207-4adb-ec36-774abb778bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.14577682456620103\n",
            "0.14577682456620103\n",
            "1.3530843112619095e-16\n"
          ]
        }
      ],
      "source": [
        "# Note: Above Assert worked, hence no Errors were returned! We'll check the values anyways\n",
        "\n",
        "print(test_f1(ValueFwd(1, 1)).d)\n",
        "print(test_df1(1))\n",
        "print(test_f1(ValueFwd(2, 1)).d - test_df1(2))\n",
        "\n",
        "# we see they're as close to each other in the order of a -16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abc2f1aa-8220-4c9e-b72a-c1abd715067d",
      "metadata": {
        "id": "abc2f1aa-8220-4c9e-b72a-c1abd715067d"
      },
      "source": [
        "# reverse mode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a10f19b-b72b-4b3d-95f6-5f95c76cc693",
      "metadata": {
        "id": "9a10f19b-b72b-4b3d-95f6-5f95c76cc693"
      },
      "source": [
        "### Reverse-mode\n",
        "\n",
        "We do two passes in the reverse mode of Automatic Differentiation. One forward and one backward.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "Reverse mode is a little more complicated than the forward mode. Here gradients are accumulated during the backward pass. so, we also have to store the computation DAG(Directed Acyclic Graph) so that the backward pass can happen in the correct order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b1c44fb3-b00f-4867-af43-595e8a4ab3ab",
      "metadata": {
        "id": "b1c44fb3-b00f-4867-af43-595e8a4ab3ab"
      },
      "outputs": [],
      "source": [
        "class ValueBwd:\n",
        "    \"\"\" stores a single scalar value and its gradient, updates gradient through back-prop \"\"\"\n",
        "    def __init__(self, data, prev = []):\n",
        "        \"\"\" Value datatype that supports backpropagation on a single scalar variable.\n",
        "\n",
        "        Args:\n",
        "            data (Number): value of the variable/constant.\n",
        "            prev (List[Tuple[ValueBwd, Number]]): List of node(ValueBwd), local gradient tuples that the current node depends on.\n",
        "                Empty list if the computational graph contains a single node.\n",
        "        Returns:\n",
        "            ValueBwd.\n",
        "        \"\"\"\n",
        "        self.v = data\n",
        "        self.d = 0\n",
        "        self._prev = prev\n",
        "\n",
        "    def __add__(l, r):\n",
        "        \"\"\" support for + on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "            l (ValueBwd): ValueBwd to the left of +.\n",
        "            r (ValueBwd): ValueBwd to the right of +.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd with updated value and gradient.\n",
        "        \"\"\"\n",
        "        return ValueBwd(l.v + r.v, [(l, 1), (r, 1)])\n",
        "\n",
        "    def __sub__(l, r):\n",
        "        \"\"\" support for - on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "            l (ValueBwd): ValueBwd to the left of -.\n",
        "            r (ValueBwd): ValueBwd to the right of -.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(l.v - r.v, [(l, 1), (r, -1)])\n",
        "\n",
        "    def __mul__(l, r):\n",
        "        \"\"\" support for * on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "            l (ValueBwd): ValueBwd to the left of -.\n",
        "            r (ValueBwd): ValueBwd to the right of -.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(l.v * r.v, [(l, r.v), (r, l.v)])\n",
        "\n",
        "    def __truediv__(l, r):\n",
        "        \"\"\" support for / on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "            l (ValueBwd): ValueBwd to the left of -.\n",
        "            r (ValueBwd): ValueBwd to the right of -.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(l.v / r.v, [(l, 1/r.v), (r, -l.v/r.v**2)])\n",
        "\n",
        "    def sin(self):\n",
        "        \"\"\" support for sin(self) on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(math.sin(self.v), [(self, math.cos(self.v))])\n",
        "\n",
        "    def cos(self):\n",
        "        \"\"\" support for cos(self) on ValueBwd datatype.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(math.cos(self.v), [(self, -math.sin(self.v))])\n",
        "\n",
        "    def exp(self, b):\n",
        "        \"\"\" support for b^self on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "            b (Number):  A numerical value.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(b**self.v, [(self, b**self.v*math.log(b))])\n",
        "\n",
        "    def pow(self, p):\n",
        "        \"\"\" support for self^p on ValueBwd datatype.\n",
        "\n",
        "        Args:\n",
        "            p (ValueBwd):  A numerical value.\n",
        "\n",
        "        Returns:\n",
        "            ValueBwd: Current node of the computational graph.\n",
        "        \"\"\"\n",
        "        return ValueBwd(self.v**p, [(self, p * (self.v**(p-1)))])\n",
        "\n",
        "    def backward(self, d=1.0):\n",
        "      \"\"\"\n",
        "      Updates local gradient and recursively calls backward on self._prev.\n",
        "\n",
        "      Args:\n",
        "          d (Number): The gradient value flowing back from the child node.\n",
        "                      Defaults to 1.0 for the final output node.\n",
        "      \"\"\"\n",
        "      self.d += d\n",
        "\n",
        "      for parent_node, local_gradient in self._prev:\n",
        "          gradient_to_pass_back = d * local_gradient\n",
        "          parent_node.backward(gradient_to_pass_back)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"v: {self.v}, d:{self.d}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86c141b6-378a-42d8-9605-4e0f14396ec4",
      "metadata": {
        "id": "86c141b6-378a-42d8-9605-4e0f14396ec4"
      },
      "source": [
        "## Test reverse mode AD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d366ff68-d0a7-45c7-9f7b-db2cdccdf115",
      "metadata": {
        "id": "d366ff68-d0a7-45c7-9f7b-db2cdccdf115"
      },
      "outputs": [],
      "source": [
        "def test_f1(x):\n",
        "    return ((x.exp(math.e) + ValueBwd(2)).pow(0.5)/ValueBwd(2)).sin()\n",
        "\n",
        "def test_df1(x):\n",
        "    ex = math.exp(x)\n",
        "    num = ex*cos(math.sqrt(ex+2)/2)\n",
        "    den = 4 * math.sqrt(ex+2)\n",
        "    return num/den"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3cd93c6e-6f88-43ab-a902-be372b33f2f5",
      "metadata": {
        "id": "3cd93c6e-6f88-43ab-a902-be372b33f2f5"
      },
      "outputs": [],
      "source": [
        "x = 2\n",
        "inp = ValueBwd(x)\n",
        "o = test_f1(inp)\n",
        "o.backward(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a1d4033a-1d9a-4f2a-8ed8-916ff9666448",
      "metadata": {
        "id": "a1d4033a-1d9a-4f2a-8ed8-916ff9666448"
      },
      "outputs": [],
      "source": [
        "assert_almost_equal(inp.d, test_df1(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c0d7447b-5bce-4c8c-a8da-839d56692701",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0d7447b-5bce-4c8c-a8da-839d56692701",
        "outputId": "e890e7d6-dca4-49eb-d261-2650b43ffc95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.3530843112619095e-16\n"
          ]
        }
      ],
      "source": [
        "print(test_df1(2) - inp.d)\n",
        "\n",
        "# Assert returns no issues\n",
        "# Same as above, they indeed are \",close enough\", hence our back prop works perfectly"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
